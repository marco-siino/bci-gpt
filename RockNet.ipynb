{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marco-siino/bci-gpt/blob/main/RockNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries and Git."
      ],
      "metadata": {
        "id": "x01Qq3k6UlZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "!git clone https://github.com/marco-siino/EEG-ATCNet.git\n",
        "!pip install mne\n",
        "!pip install PyWavelets\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import time\n",
        "import pywt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "os.chdir(\"/content/EEG-ATCNet\")\n",
        "\n",
        "import models\n",
        "from preprocess import get_data\n",
        "# from keras.utils.vis_utils import plot_model"
      ],
      "metadata": {
        "id": "kExkM1UKKKJ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1c6b9a-20d1-448f-f9b8-254c3ee72d6e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EEG-ATCNet'...\n",
            "remote: Enumerating objects: 284, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 284 (delta 105), reused 42 (delta 42), pack-reused 163 (from 1)\u001b[K\n",
            "Receiving objects: 100% (284/284), 15.96 MiB | 19.00 MiB/s, done.\n",
            "Resolving deltas: 100% (138/138), done.\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.5)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from mne) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (24.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from mne) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.1.31)\n",
            "Collecting PyWavelets\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (1.26.4)\n",
            "Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets\n",
            "Successfully installed PyWavelets-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Organize the dataset."
      ],
      "metadata": {
        "id": "anMFWoueUv95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creare la cartella \"dataset\" e \"results\" se non esistono già\n",
        "os.makedirs(\"dataset\", exist_ok=True)\n",
        "\n",
        "# Creare le cartelle da s1 a s9\n",
        "for i in range(1, 10):\n",
        "    os.makedirs(f\"dataset/s{i}\", exist_ok=True)"
      ],
      "metadata": {
        "id": "PM6WBGY4fBe9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset.\n",
        "base_url = \"https://bnci-horizon-2020.eu/database/data-sets/001-2014/\"\n",
        "save_path = \"dataset/\"\n",
        "\n",
        "for i in range(1, 10):  # Da A01 a A09\n",
        "    for suffix in [\"T\", \"E\"]:  # T per training, E per evaluation\n",
        "        filename = f\"A{i:02d}{suffix}.mat\"\n",
        "        url = base_url + filename\n",
        "        os.system(f\"wget -O {save_path}s{i}/{filename} {url}\")"
      ],
      "metadata": {
        "id": "KrRurPbzKuRH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions definitions"
      ],
      "metadata": {
        "id": "lKIHsRuSU-zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "def draw_learning_curves(history, sub):\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Model accuracy - subject: ' + str(sub))\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'val'], loc='upper left')\n",
        "    plt.show()\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss - subject: ' + str(sub))\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'val'], loc='upper left')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def draw_confusion_matrix(cf_matrix, sub, results_path, classes_labels):\n",
        "    # Generate confusion matrix plot\n",
        "    display_labels = classes_labels\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix,\n",
        "                                display_labels=display_labels)\n",
        "    disp.plot()\n",
        "    disp.ax_.set_xticklabels(display_labels, rotation=12)\n",
        "    plt.title('Confusion Matrix of Subject: ' + sub )\n",
        "    plt.savefig(results_path + '/subject_' + sub + '.png')\n",
        "    plt.show()\n",
        "\n",
        "def draw_performance_barChart(num_sub, metric, label):\n",
        "    fig, ax = plt.subplots()\n",
        "    x = list(range(1, num_sub+1))\n",
        "    ax.bar(x, metric, 0.5, label=label)\n",
        "    ax.set_ylabel(label)\n",
        "    ax.set_xlabel(\"Subject\")\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_title('Model '+ label + ' per subject')\n",
        "    ax.set_ylim([0,1])"
      ],
      "metadata": {
        "id": "n6r2i2nHVDHa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing functions definition"
      ],
      "metadata": {
        "id": "byiRKS6cVQpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DB4 (Soft -> Threshold 5.5)"
      ],
      "metadata": {
        "id": "JBbRpj7xW3P4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def db4_soft(signal, wavelet='db4', level=4, threshold=5.5):\n",
        "    \"\"\"\n",
        "    Applica la RDWT (Discrete Wavelet Transform) al segnale e lo ricostruisce,\n",
        "    rimuovendo i coefficienti di dettaglio sotto una certa soglia per enfatizzare\n",
        "    le caratteristiche principali del segnale.\n",
        "\n",
        "    Args:\n",
        "    - signal: il segnale da elaborare\n",
        "    - wavelet: tipo di wavelet da utilizzare (default 'db4')\n",
        "    - level: livello della decomposizione (default 4)\n",
        "    - threshold: soglia per i coefficienti di dettaglio (default 0.5)\n",
        "\n",
        "    Returns:\n",
        "    - Il segnale ricostruito dopo la manipolazione della RDWT\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    print(\"Lunghezza originale:\", signal.shape[-1])  # Dovrebbe essere 1125\n",
        "    coeffs = pywt.wavedec(signal, wavelet, mode='per', level=level)\n",
        "    print(\"Lunghezza coefficiente di approssimazione:\", len(coeffs[0]))\n",
        "\n",
        "    reconstructed_signal = pywt.waverec(coeffs, wavelet, mode='per')\n",
        "    print(\"Lunghezza ricostruita:\", len(reconstructed_signal))\n",
        "\n",
        "    # Decomposizione del segnale in coefficienti\n",
        "    coeffs = pywt.wavedec(signal, wavelet, mode='per', level=level)\n",
        "\n",
        "    # Modifica i coefficienti di dettaglio\n",
        "    # Si applica un threshold sui coefficienti di dettaglio per \"ridurre\" la componente di alta frequenza\n",
        "    coeffs_thresholded = [coeffs[0]]  # Mantieni il coefficiente di approssimazione\n",
        "    for i in range(1, len(coeffs)):\n",
        "        coeffs_thresholded.append(np.where(np.abs(coeffs[i]) < threshold, 0, coeffs[i]))\n",
        "\n",
        "    # Ricostruzione del segnale dai coefficienti modificati\n",
        "    reconstructed_signal = pywt.waverec(coeffs_thresholded, wavelet, mode='per')\n",
        "\n",
        "    # Taglia il segnale ricostruito per mantenerne la stessa lunghezza dell'input\n",
        "    return reconstructed_signal[:len(signal)]\n"
      ],
      "metadata": {
        "id": "lUw3WgVjXEb6"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DB4 (Hard -> Threshold 10.5)"
      ],
      "metadata": {
        "id": "U_7jS5mAW7X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def db4_hard(signal, wavelet='db4', level=4, threshold=10.5):\n",
        "    \"\"\"\n",
        "    Applica la RDWT (Discrete Wavelet Transform) al segnale e lo ricostruisce,\n",
        "    rimuovendo i coefficienti di dettaglio sotto una certa soglia per enfatizzare\n",
        "    le caratteristiche principali del segnale.\n",
        "\n",
        "    Args:\n",
        "    - signal: il segnale da elaborare\n",
        "    - wavelet: tipo di wavelet da utilizzare (default 'db4')\n",
        "    - level: livello della decomposizione (default 4)\n",
        "    - threshold: soglia per i coefficienti di dettaglio (default 0.5)\n",
        "\n",
        "    Returns:\n",
        "    - Il segnale ricostruito dopo la manipolazione della RDWT\n",
        "    \"\"\"\n",
        "\n",
        "    # Decomposizione del segnale in coefficienti\n",
        "    coeffs = pywt.wavedec(signal, wavelet, mode='per', level=level)\n",
        "\n",
        "    # Modifica i coefficienti di dettaglio\n",
        "    # Si applica un threshold sui coefficienti di dettaglio per \"ridurre\" la componente di alta frequenza\n",
        "    coeffs_thresholded = [coeffs[0]]  # Mantieni il coefficiente di approssimazione\n",
        "    for i in range(1, len(coeffs)):\n",
        "        coeffs_thresholded.append(np.where(np.abs(coeffs[i]) < threshold, 0, coeffs[i]))\n",
        "\n",
        "    # Ricostruzione del segnale dai coefficienti modificati\n",
        "    reconstructed_signal = pywt.waverec(coeffs_thresholded, wavelet, mode='per')\n",
        "\n",
        "    # Taglia il segnale ricostruito per mantenerne la stessa lunghezza dell'input\n",
        "    return reconstructed_signal[:len(signal)]\n"
      ],
      "metadata": {
        "id": "lQDSj1UhXQbo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDWT"
      ],
      "metadata": {
        "id": "1NG7RQTKWZjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.signal as signal\n",
        "import pywt\n",
        "\n",
        "def rational_dilated_wavelet_transform(sig, wavelet='db4', levels=4, dilation_factors=None, threshold=5.5):\n",
        "    \"\"\"\n",
        "    Applica la Rational Dilated Wavelet Transform (RDWT) al segnale, eseguendo una decomposizione con dilatazione razionale.\n",
        "\n",
        "    Args:\n",
        "    - sig: il segnale da elaborare (numpy array)\n",
        "    - wavelet: tipo di wavelet da utilizzare (default 'db4')\n",
        "    - levels: numero di livelli di decomposizione (default 4)\n",
        "    - dilation_factors: fattori di dilatazione razionale per ogni livello (es. [3/2, 5/3, 7/4, ...])\n",
        "    - threshold: soglia per i coefficienti di dettaglio (default 5.5)\n",
        "\n",
        "    Returns:\n",
        "    - Il segnale ricostruito dopo la manipolazione della RDWT\n",
        "    \"\"\"\n",
        "\n",
        "    if dilation_factors is None:\n",
        "        # Se non specificati, usa fattori di scala crescenti\n",
        "        dilation_factors = [3/2, 5/3, 7/4, 9/5]\n",
        "\n",
        "    coeffs_approx = sig  # Inizializza con il segnale originale\n",
        "    detail_coeffs = []\n",
        "\n",
        "    for i in range(levels):\n",
        "        factor = dilation_factors[i]\n",
        "\n",
        "        # Applica un filtro wavelet con fattore di dilatazione razionale\n",
        "        wavelet_filter = pywt.Wavelet(wavelet)\n",
        "        lo_d, hi_d = wavelet_filter.dec_lo, wavelet_filter.dec_hi\n",
        "\n",
        "        # Ridimensiona i filtri per rispettare la scala razionale\n",
        "        lo_d = signal.resample(lo_d, int(len(lo_d) * factor))\n",
        "        hi_d = signal.resample(hi_d, int(len(hi_d) * factor))\n",
        "\n",
        "        # Convoluzione per decomposizione\n",
        "        approx = np.convolve(coeffs_approx, lo_d, mode='same')\n",
        "        detail = np.convolve(coeffs_approx, hi_d, mode='same')\n",
        "\n",
        "        # Soglia i coefficienti di dettaglio\n",
        "        detail[np.abs(detail) < threshold] = 0\n",
        "\n",
        "        # Salva il dettaglio e passa l'approssimazione al livello successivo\n",
        "        detail_coeffs.append(detail)\n",
        "        coeffs_approx = approx\n",
        "\n",
        "    # Ricostruzione del segnale\n",
        "    reconstructed = coeffs_approx\n",
        "    for i in range(levels-1, -1, -1):\n",
        "        reconstructed += detail_coeffs[i]  # Somma i dettagli per la ricostruzione\n",
        "\n",
        "    return reconstructed\n"
      ],
      "metadata": {
        "id": "1l6F2E0iVovM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_preprocessing (signals_data, preprocessing):\n",
        "  if preprocessing==\"none\":\n",
        "    return signals_data\n",
        "  elif preprocessing==\"db4_soft\":\n",
        "    return db4_soft(signals_data)\n",
        "  elif preprocessing==\"db4_hard\":\n",
        "    return db4_hard(signals_data)\n",
        "  elif preprocessing==\"rdwt\":\n",
        "    return rational_dilated_wavelet_transform(signals_data)\n"
      ],
      "metadata": {
        "id": "8AAXepUzYZOv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training function"
      ],
      "metadata": {
        "id": "LcIk3eTxUeqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Training\n",
        "def train(dataset_conf, train_conf, results_path):\n",
        "\n",
        "    # remove the 'result' folder before training\n",
        "    if os.path.exists(results_path):\n",
        "        # Remove the folder and its contents\n",
        "        shutil.rmtree(results_path)\n",
        "        os.makedirs(results_path)\n",
        "\n",
        "    # Get the current 'IN' time to calculate the overall training time\n",
        "    in_exp = time.time()\n",
        "    # Create a file to store the path of the best model among several runs\n",
        "    best_models = open(results_path + \"/best models.txt\", \"w\")\n",
        "    # Create a file to store performance during training\n",
        "    log_write = open(results_path + \"/log.txt\", \"w\")\n",
        "\n",
        "    # Get dataset parameters\n",
        "    dataset = dataset_conf.get('name')\n",
        "    n_sub = dataset_conf.get('n_sub')\n",
        "    data_path = dataset_conf.get('data_path')\n",
        "    isStandard = dataset_conf.get('isStandard')\n",
        "    LOSO = dataset_conf.get('LOSO')\n",
        "    # Get training hyperparamters\n",
        "    batch_size = train_conf.get('batch_size')\n",
        "    epochs = train_conf.get('epochs')\n",
        "    signal_preprocessing = dataset_conf.get('signal_preprocessing')\n",
        "    patience = train_conf.get('patience')\n",
        "    lr = train_conf.get('lr')\n",
        "    LearnCurves = train_conf.get('LearnCurves') # Plot Learning Curves?\n",
        "    n_train = train_conf.get('n_train')\n",
        "    model_name = train_conf.get('model')\n",
        "    from_logits = train_conf.get('from_logits')\n",
        "\n",
        "    # Initialize variables\n",
        "    acc = np.zeros((n_sub, n_train))\n",
        "    kappa = np.zeros((n_sub, n_train))\n",
        "\n",
        "    # Iteration over subjects\n",
        "    # for sub in range(n_sub-1, n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
        "    for sub in range(n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
        "\n",
        "        print('\\nTraining on subject ', sub+1)\n",
        "        log_write.write( '\\nTraining on subject '+ str(sub+1) +'\\n')\n",
        "        # Initiating variables to save the best subject accuracy among multiple runs.\n",
        "        BestSubjAcc = 0\n",
        "        bestTrainingHistory = []\n",
        "\n",
        "        # Get training and validation data\n",
        "        X_train, _, y_train_onehot, _, _, _ = get_data(\n",
        "            data_path, sub, dataset, LOSO = LOSO, isStandard = isStandard)\n",
        "\n",
        "        # Divide the training data into training and validation\n",
        "        X_train, X_val, y_train_onehot, y_val_onehot = train_test_split(X_train, y_train_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "        print(\"\\n\\nBefore preprocessing X_train shape is:\"+str(X_train.shape))\n",
        "        print(\"Before preprocessing X_val shape is:\"+str(X_val.shape))\n",
        "\n",
        "        X_train = apply_preprocessing(X_train, preprocessing=signal_preprocessing)\n",
        "        X_val = apply_preprocessing(X_val, preprocessing=signal_preprocessing)\n",
        "        print(\"\\n\\nAfter preprocessing X_train shape is:\"+str(X_train.shape))\n",
        "        print(\"After preprocessing X_val shape is:\"+str(X_val.shape))\n",
        "\n",
        "\n",
        "        # Iteration over multiple runs\n",
        "        for train in range(n_train): # How many repetitions of training for subject i.\n",
        "            # Set the random seed for TensorFlow and NumPy random number generator.\n",
        "            # The purpose of setting a seed is to ensure reproducibility in random operations.\n",
        "            tf.random.set_seed(train+1)\n",
        "            np.random.seed(train+1)\n",
        "\n",
        "            # Get the current 'IN' time to calculate the 'run' training time\n",
        "            in_run = time.time()\n",
        "\n",
        "            # Create folders and files to save trained models for all runs\n",
        "            filepath = results_path + '/saved models/run-{}'.format(train+1)\n",
        "            if not os.path.exists(filepath):\n",
        "                os.makedirs(filepath)\n",
        "            filepath = filepath + '/subject-{}.weights.h5'.format(sub+1)\n",
        "\n",
        "            # Create the model\n",
        "            model = getModel(model_name, dataset_conf, from_logits)\n",
        "            # Compile and train the model\n",
        "            model.compile(loss=CategoricalCrossentropy(from_logits=from_logits), optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n",
        "\n",
        "            # model.summary()\n",
        "            # plot_model(model, to_file='plot_model.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "            callbacks = [\n",
        "                ModelCheckpoint(filepath, monitor='val_loss', verbose=0,\n",
        "                                save_best_only=True, save_weights_only=True, mode='min'),\n",
        "                ReduceLROnPlateau(monitor=\"val_loss\", factor=0.90, patience=20, verbose=0, min_lr=0.0001),\n",
        "                # EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=patience)\n",
        "            ]\n",
        "            history = model.fit(X_train, y_train_onehot, validation_data=(X_val, y_val_onehot),\n",
        "                                epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=0)\n",
        "\n",
        "            # Evaluate the performance of the trained model based on the validation data\n",
        "            # Here we load the Trained weights from the file saved in the hard\n",
        "            # disk, which should be the same as the weights of the current model.\n",
        "            model.load_weights(filepath)\n",
        "            y_pred = model.predict(X_val)\n",
        "\n",
        "            if from_logits:\n",
        "                y_pred = tf.nn.softmax(y_pred).numpy().argmax(axis=-1)\n",
        "            else:\n",
        "                y_pred = y_pred.argmax(axis=-1)\n",
        "\n",
        "            labels = y_val_onehot.argmax(axis=-1)\n",
        "            acc[sub, train]  = accuracy_score(labels, y_pred)\n",
        "            kappa[sub, train] = cohen_kappa_score(labels, y_pred)\n",
        "\n",
        "            # Get the current 'OUT' time to calculate the 'run' training time\n",
        "            out_run = time.time()\n",
        "            # Print & write performance measures for each run\n",
        "            info = 'Subject: {}   seed {}   time: {:.1f} m   '.format(sub+1, train+1, ((out_run-in_run)/60))\n",
        "            info = info + 'valid_acc: {:.4f}   valid_loss: {:.3f}'.format(acc[sub, train], min(history.history['val_loss']))\n",
        "            print(info)\n",
        "            log_write.write(info +'\\n')\n",
        "            # If current training run is better than previous runs, save the history.\n",
        "            if(BestSubjAcc < acc[sub, train]):\n",
        "                 BestSubjAcc = acc[sub, train]\n",
        "                 bestTrainingHistory = history\n",
        "\n",
        "        # Store the path of the best model among several runs\n",
        "        best_run = np.argmax(acc[sub,:])\n",
        "        filepath = '/saved models/run-{}/subject-{}.h5'.format(best_run+1, sub+1)+'\\n'\n",
        "        best_models.write(filepath)\n",
        "\n",
        "        # Plot Learning curves\n",
        "        if (LearnCurves == True):\n",
        "            print('Plot Learning Curves ....... ')\n",
        "            draw_learning_curves(bestTrainingHistory, sub+1)\n",
        "\n",
        "    # Get the current 'OUT' time to calculate the overall training time\n",
        "    out_exp = time.time()\n",
        "\n",
        "    # Print & write the validation performance using all seeds\n",
        "    head1 = head2 = '         '\n",
        "    for sub in range(n_sub):\n",
        "        head1 = head1 + 'sub_{}   '.format(sub+1)\n",
        "        head2 = head2 + '-----   '\n",
        "    head1 = head1 + '  average'\n",
        "    head2 = head2 + '  -------'\n",
        "    info = '\\n---------------------------------\\nValidation performance (acc %):'\n",
        "    info = info + '\\n---------------------------------\\n' + head1 +'\\n'+ head2\n",
        "    for run in range(n_train):\n",
        "        info = info + '\\nSeed {}:  '.format(run+1)\n",
        "        for sub in range(n_sub):\n",
        "            info = info + '{:.2f}   '.format(acc[sub, run]*100)\n",
        "        info = info + '  {:.2f}   '.format(np.average(acc[:, run])*100)\n",
        "    info = info + '\\n---------------------------------\\nAverage acc - all seeds: '\n",
        "    info = info + '{:.2f} %\\n\\nTrain Time  - all seeds: {:.1f}'.format(np.average(acc)*100, (out_exp-in_exp)/(60))\n",
        "    info = info + ' min\\n---------------------------------\\n'\n",
        "    print(info)\n",
        "    log_write.write(info+'\\n')\n",
        "\n",
        "    # Close open files\n",
        "    best_models.close()\n",
        "    log_write.close()\n"
      ],
      "metadata": {
        "id": "lUYrFX-SUdQP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation function"
      ],
      "metadata": {
        "id": "94bvHJeiVHbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Evaluation\n",
        "def test(model, dataset_conf, results_path, allRuns = True):\n",
        "    # Open the  \"Log\" file to write the evaluation results\n",
        "    log_write = open(results_path + \"/log.txt\", \"a\")\n",
        "\n",
        "    # Get dataset paramters\n",
        "    dataset = dataset_conf.get('name')\n",
        "    signal_preprocessing = dataset_conf.get('signal_preprocessing')\n",
        "    n_classes = dataset_conf.get('n_classes')\n",
        "    n_sub = dataset_conf.get('n_sub')\n",
        "    data_path = dataset_conf.get('data_path')\n",
        "    isStandard = dataset_conf.get('isStandard')\n",
        "    LOSO = dataset_conf.get('LOSO')\n",
        "    classes_labels = dataset_conf.get('cl_labels')\n",
        "\n",
        "    # Test the performance based on several runs (seeds)\n",
        "    runs = os.listdir(results_path+\"/saved models\")\n",
        "    # Initialize variables\n",
        "    acc = np.zeros((n_sub, len(runs)))\n",
        "    kappa = np.zeros((n_sub, len(runs)))\n",
        "    cf_matrix = np.zeros([n_sub, len(runs), n_classes, n_classes])\n",
        "\n",
        "    # Iteration over subjects\n",
        "    # for sub in range(n_sub-1, n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
        "    inference_time = 0 #  inference_time: classification time for one trial\n",
        "    for sub in range(n_sub): # (num_sub): for all subjects, (i-1,i): for the ith subject.\n",
        "        # Load data\n",
        "        _, _, _, X_test, _, y_test_onehot = get_data(data_path, sub, dataset, LOSO = LOSO, isStandard = isStandard)\n",
        "\n",
        "        X_test = apply_preprocessing(X_test, preprocessing=\"rdwt\")\n",
        "\n",
        "        # Iteration over runs (seeds)\n",
        "        for seed in range(len(runs)):\n",
        "            # Load the model of the seed.\n",
        "            model.load_weights('{}/saved models/{}/subject-{}.h5'.format(results_path, runs[seed], sub+1))\n",
        "\n",
        "            inference_time = time.time()\n",
        "            # Predict MI task\n",
        "            y_pred = model.predict(X_test).argmax(axis=-1)\n",
        "            inference_time = (time.time() - inference_time)/X_test.shape[0]\n",
        "            # Calculate accuracy and K-score\n",
        "            labels = y_test_onehot.argmax(axis=-1)\n",
        "            acc[sub, seed]  = accuracy_score(labels, y_pred)\n",
        "            kappa[sub, seed] = cohen_kappa_score(labels, y_pred)\n",
        "            # Calculate and draw confusion matrix\n",
        "            cf_matrix[sub, seed, :, :] = confusion_matrix(labels, y_pred, normalize='true')\n",
        "            # draw_confusion_matrix(cf_matrix[sub, seed, :, :], str(sub+1), results_path, classes_labels)\n",
        "\n",
        "    # Print & write the average performance measures for all subjects\n",
        "    head1 = head2 = '                  '\n",
        "    for sub in range(n_sub):\n",
        "        head1 = head1 + 'sub_{}   '.format(sub+1)\n",
        "        head2 = head2 + '-----   '\n",
        "    head1 = head1 + '  average'\n",
        "    head2 = head2 + '  -------'\n",
        "    info = '\\n' + head1 +'\\n'+ head2\n",
        "    info = '\\n---------------------------------\\nTest performance (acc & k-score):\\n'\n",
        "    info = info + '---------------------------------\\n' + head1 +'\\n'+ head2\n",
        "    for run in range(len(runs)):\n",
        "        info = info + '\\nSeed {}: '.format(run+1)\n",
        "        info_acc = '(acc %)   '\n",
        "        info_k = '        (k-sco)   '\n",
        "        for sub in range(n_sub):\n",
        "            info_acc = info_acc + '{:.2f}   '.format(acc[sub, run]*100)\n",
        "            info_k = info_k + '{:.3f}   '.format(kappa[sub, run])\n",
        "        info_acc = info_acc + '  {:.2f}   '.format(np.average(acc[:, run])*100)\n",
        "        info_k = info_k + '  {:.3f}   '.format(np.average(kappa[:, run]))\n",
        "        info = info + info_acc + '\\n' + info_k\n",
        "    info = info + '\\n----------------------------------\\nAverage - all seeds (acc %): '\n",
        "    info = info + '{:.2f}\\n                    (k-sco): '.format(np.average(acc)*100)\n",
        "    info = info + '{:.3f}\\n\\nInference time: {:.2f}'.format(np.average(kappa), inference_time * 1000)\n",
        "    info = info + ' ms per trial\\n----------------------------------\\n'\n",
        "    print(info)\n",
        "    log_write.write(info+'\\n')\n",
        "\n",
        "    # Draw a performance bar chart for all subjects\n",
        "    draw_performance_barChart(n_sub, acc.mean(1), 'Accuracy')\n",
        "    draw_performance_barChart(n_sub, kappa.mean(1), 'k-score')\n",
        "    # Draw confusion matrix for all subjects (average)\n",
        "    draw_confusion_matrix(cf_matrix.mean((0,1)), 'All', results_path, classes_labels)\n",
        "    # Close opened file\n",
        "    log_write.close()\n"
      ],
      "metadata": {
        "id": "doqjxvQFVGeB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model selection and setup"
      ],
      "metadata": {
        "id": "weCl_i-ZVdlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "def getModel(model_name, dataset_conf, from_logits = False):\n",
        "\n",
        "    n_classes = dataset_conf.get('n_classes')\n",
        "    n_channels = dataset_conf.get('n_channels')\n",
        "    in_samples = dataset_conf.get('in_samples')\n",
        "\n",
        "    # Select the model\n",
        "    if(model_name == 'RockNet'):\n",
        "        # Train using the proposed ATCNet model: https://ieeexplore.ieee.org/document/9852687\n",
        "        model = models.RockNet_(\n",
        "            # Dataset parameters\n",
        "            n_classes = n_classes,\n",
        "            in_chans = n_channels,\n",
        "            in_samples = in_samples,\n",
        "            # Sliding window (SW) parameter\n",
        "            n_windows = 5,\n",
        "            # Attention (AT) block parameter\n",
        "            attention = 'mha', # Options: None, 'mha','mhla', 'cbam', 'se'\n",
        "            # Convolutional (CV) block parameters\n",
        "            eegn_F1 = 16,\n",
        "            eegn_D = 2,\n",
        "            eegn_kernelSize = 64,\n",
        "            eegn_poolSize = 7,\n",
        "            eegn_dropout = 0.5,\n",
        "            # Temporal convolutional (TC) block parameters\n",
        "            tcn_depth = 2,\n",
        "            tcn_kernelSize = 4,\n",
        "            tcn_filters = 64,\n",
        "            tcn_dropout = 0.3,\n",
        "            tcn_activation='elu',\n",
        "            )\n",
        "    elif(model_name == 'ATCNet'):\n",
        "        # Train using the proposed ATCNet model: https://ieeexplore.ieee.org/document/9852687\n",
        "        model = models.ATCNet_(\n",
        "            # Dataset parameters\n",
        "            n_classes = n_classes,\n",
        "            in_chans = n_channels,\n",
        "            in_samples = in_samples,\n",
        "            # Sliding window (SW) parameter\n",
        "            n_windows = 5,\n",
        "            # Attention (AT) block parameter\n",
        "            attention = 'mha', # Options: None, 'mha','mhla', 'cbam', 'se'\n",
        "            # Convolutional (CV) block parameters\n",
        "            eegn_F1 = 16,\n",
        "            eegn_D = 2,\n",
        "            eegn_kernelSize = 64,\n",
        "            eegn_poolSize = 7,\n",
        "            eegn_dropout = 0.3,\n",
        "            # Temporal convolutional (TC) block parameters\n",
        "            tcn_depth = 2,\n",
        "            tcn_kernelSize = 4,\n",
        "            tcn_filters = 32,\n",
        "            tcn_dropout = 0.3,\n",
        "            tcn_activation='elu',\n",
        "            )\n",
        "    elif(model_name == 'TCNet_Fusion'):\n",
        "        # Train using TCNet_Fusion: https://doi.org/10.1016/j.bspc.2021.102826\n",
        "        model = models.TCNet_Fusion(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n",
        "    elif(model_name == 'EEGTCNet'):\n",
        "        # Train using EEGTCNet: https://arxiv.org/abs/2006.00622\n",
        "        model = models.EEGTCNet(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n",
        "    elif(model_name == 'EEGNet'):\n",
        "        # Train using EEGNet: https://arxiv.org/abs/1611.08024\n",
        "        model = models.EEGNet_classifier(n_classes = n_classes, Chans=n_channels, Samples=in_samples)\n",
        "    elif(model_name == 'EEGNeX'):\n",
        "        # Train using EEGNeX: https://arxiv.org/abs/2207.12369\n",
        "        model = models.EEGNeX_8_32(n_timesteps = in_samples , n_features = n_channels, n_outputs = n_classes)\n",
        "    elif(model_name == 'DeepConvNet'):\n",
        "        # Train using DeepConvNet: https://doi.org/10.1002/hbm.23730\n",
        "        model = models.DeepConvNet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
        "    elif(model_name == 'ShallowConvNet'):\n",
        "        # Train using ShallowConvNet: https://doi.org/10.1002/hbm.23730\n",
        "        model = models.ShallowConvNet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
        "    elif(model_name == 'MBEEG_SENet'):\n",
        "        # Train using MBEEG_SENet: https://www.mdpi.com/2075-4418/12/4/995\n",
        "        model = models.MBEEG_SENet(nb_classes = n_classes , Chans = n_channels, Samples = in_samples)\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"'{}' model is not supported yet!\".format(model_name))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "5gAM65lQSTH1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the simulation"
      ],
      "metadata": {
        "id": "PE_qndEAVZ-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%\n",
        "def run():\n",
        "    # Define dataset parameters\n",
        "    dataset = 'BCI2a' # Options: 'BCI2a','HGD', 'CS2R'\n",
        "\n",
        "    if dataset == 'BCI2a':\n",
        "        in_samples = 1125\n",
        "        n_channels = 22\n",
        "        n_sub = 9\n",
        "        n_classes = 4\n",
        "        classes_labels = ['Left hand', 'Right hand','Foot','Tongue']\n",
        "        data_path = 'dataset/' #os.path.expanduser('~') + '/BCI Competition IV/BCI Competition IV-2a/BCI Competition IV 2a mat/'\n",
        "    elif dataset == 'HGD':\n",
        "        in_samples = 1125\n",
        "        n_channels = 44\n",
        "        n_sub = 14\n",
        "        n_classes = 4\n",
        "        classes_labels = ['Right Hand', 'Left Hand','Rest','Feet']\n",
        "        data_path = os.path.expanduser('~') + '/mne_data/MNE-schirrmeister2017-data/robintibor/high-gamma-dataset/raw/master/data/'\n",
        "    elif dataset == 'CS2R':\n",
        "        in_samples = 1125\n",
        "        # in_samples = 576\n",
        "        n_channels = 32\n",
        "        n_sub = 18\n",
        "        n_classes = 3\n",
        "        # classes_labels = ['Fingers', 'Wrist','Elbow','Rest']\n",
        "        classes_labels = ['Fingers', 'Wrist','Elbow']\n",
        "        # classes_labels = ['Fingers', 'Elbow']\n",
        "        data_path = os.path.expanduser('~') + '/CS2R MI EEG dataset/all/EDF - Cleaned - phase one (remove extra runs)/two sessions/'\n",
        "    else:\n",
        "        raise Exception(\"'{}' dataset is not supported yet!\".format(dataset))\n",
        "\n",
        "    # Create a folder to store the results of the experiment\n",
        "    results_path = os.getcwd() + \"/results\"\n",
        "    if not  os.path.exists(results_path):\n",
        "      os.makedirs(results_path)   # Create a new directory if it does not exist\n",
        "\n",
        "    # Set dataset paramters\n",
        "    dataset_conf = { 'name': dataset, 'n_classes': n_classes, 'cl_labels': classes_labels,\n",
        "                    'n_sub': n_sub, 'n_channels': n_channels, 'in_samples': in_samples,\n",
        "                    'data_path': data_path, 'isStandard': True, 'LOSO': False,\n",
        "                     'signal_preprocessing':\"db4_soft\"}\n",
        "\n",
        "    # Set training hyperparamters\n",
        "    train_conf = { 'batch_size': 64, 'epochs': 500, 'patience': 100, 'lr': 0.001,'n_train': 1,\n",
        "                  'LearnCurves': True, 'from_logits': False, 'model':'RockNet'}\n",
        "\n",
        "    # Train the model\n",
        "    train(dataset_conf, train_conf, results_path)\n",
        "\n",
        "    # Evaluate the model based on the weights saved in the '/results' folder\n",
        "    model = getModel(train_conf.get('model'), dataset_conf)\n",
        "    test(model, dataset_conf, results_path)\n",
        "\n",
        "#%%\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n"
      ],
      "metadata": {
        "id": "nWF0BbQzVbdN",
        "outputId": "a12d0e5d-c717-4ed2-f0e6-c5a61df955e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on subject  1\n",
            "\n",
            "\n",
            "Before preprocessing X_train shape is:(230, 1, 22, 1125)\n",
            "Before preprocessing X_val shape is:(58, 1, 22, 1125)\n",
            "Lunghezza originale: 1125\n",
            "Lunghezza coefficiente di approssimazione: 230\n",
            "Lunghezza ricostruita: 230\n",
            "Lunghezza originale: 1125\n",
            "Lunghezza coefficiente di approssimazione: 58\n",
            "Lunghezza ricostruita: 58\n",
            "\n",
            "\n",
            "After preprocessing X_train shape is:(230, 1, 22, 1126)\n",
            "After preprocessing X_val shape is:(58, 1, 22, 1126)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input 0 of layer \"functional_8\" is incompatible with the layer: expected shape=(None, 1, 22, 1125), found shape=(None, 1, 22, 1126)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-bdb3ab61d157>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#%%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-44-bdb3ab61d157>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Evaluate the model based on the weights saved in the '/results' folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-a3bbbae7ea7a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset_conf, train_conf, results_path)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m# EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=patience)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             ]\n\u001b[0;32m---> 95\u001b[0;31m             history = model.fit(X_train, y_train_onehot, validation_data=(X_val, y_val_onehot),\n\u001b[0m\u001b[1;32m     96\u001b[0m                                 epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=0)\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mspec_dim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                         raise ValueError(\n\u001b[0m\u001b[1;32m    246\u001b[0m                             \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" is '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                             \u001b[0;34m\"incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"functional_8\" is incompatible with the layer: expected shape=(None, 1, 22, 1125), found shape=(None, 1, 22, 1126)"
          ]
        }
      ]
    }
  ]
}